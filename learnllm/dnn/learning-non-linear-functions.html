<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 3 Learning non-linear functions | Understanding Deep Learning</title>
  <meta name="description" content="Chapter 3 Learning non-linear functions | Understanding Deep Learning" />
  <meta name="generator" content="bookdown 0.12 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 3 Learning non-linear functions | Understanding Deep Learning" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="Chapter 3 Learning non-linear functions | Understanding Deep Learning" />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 3 Learning non-linear functions | Understanding Deep Learning" />
  
  <meta name="twitter:description" content="Chapter 3 Learning non-linear functions | Understanding Deep Learning" />
  

<meta name="author" content="Rajesh Talluri, PhD" />


<meta name="date" content="2019-09-12" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="the-artificial-neuron.html">
<link rel="next" href="gradient-descent-algorithms.html">
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />







<script src="libs/kePrint-0.0.1/kePrint.js"></script>


<style type="text/css">
a.sourceLine { display: inline-block; line-height: 1.25; }
a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }
a.sourceLine:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
a.sourceLine { text-indent: -1em; padding-left: 1em; }
}
pre.numberSource a.sourceLine
  { position: relative; left: -4em; }
pre.numberSource a.sourceLine::before
  { content: attr(title);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; pointer-events: all; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  }
@media screen {
a.sourceLine::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Understanding Deep Learning</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a></li>
<li class="chapter" data-level="1" data-path="start-asking-questions.html"><a href="start-asking-questions.html"><i class="fa fa-check"></i><b>1</b> Start asking questions</a><ul>
<li class="chapter" data-level="1.1" data-path="start-asking-questions.html"><a href="start-asking-questions.html#what-is-deep-learning"><i class="fa fa-check"></i><b>1.1</b> What is deep learning?</a></li>
<li class="chapter" data-level="1.2" data-path="start-asking-questions.html"><a href="start-asking-questions.html#what-are-artificial-neural-networks"><i class="fa fa-check"></i><b>1.2</b> What are artificial neural networks?</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="the-artificial-neuron.html"><a href="the-artificial-neuron.html"><i class="fa fa-check"></i><b>2</b> The artificial neuron</a><ul>
<li class="chapter" data-level="2.1" data-path="the-artificial-neuron.html"><a href="the-artificial-neuron.html#what-can-a-single-neuron-accomplish"><i class="fa fa-check"></i><b>2.1</b> What can a single neuron accomplish?</a><ul>
<li class="chapter" data-level="2.1.1" data-path="the-artificial-neuron.html"><a href="the-artificial-neuron.html#linear-regression"><i class="fa fa-check"></i><b>2.1.1</b> Linear Regression</a></li>
<li class="chapter" data-level="2.1.2" data-path="the-artificial-neuron.html"><a href="the-artificial-neuron.html#logistic-regression"><i class="fa fa-check"></i><b>2.1.2</b> Logistic regression</a></li>
<li class="chapter" data-level="2.1.3" data-path="the-artificial-neuron.html"><a href="the-artificial-neuron.html#linear-classification"><i class="fa fa-check"></i><b>2.1.3</b> Linear Classification</a></li>
<li class="chapter" data-level="2.1.4" data-path="the-artificial-neuron.html"><a href="the-artificial-neuron.html#other-activation-functions"><i class="fa fa-check"></i><b>2.1.4</b> Other activation functions</a></li>
<li class="chapter" data-level="2.1.5" data-path="the-artificial-neuron.html"><a href="the-artificial-neuron.html#biological-activation-functions"><i class="fa fa-check"></i><b>2.1.5</b> Biological Activation functions</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="the-artificial-neuron.html"><a href="the-artificial-neuron.html#how-does-a-neuron-learn"><i class="fa fa-check"></i><b>2.2</b> How does a neuron learn?</a><ul>
<li class="chapter" data-level="2.2.1" data-path="the-artificial-neuron.html"><a href="the-artificial-neuron.html#estimation-of-weights-for-a-single-neuron-with-identity-activation"><i class="fa fa-check"></i><b>2.2.1</b> Estimation of weights for a single neuron with identity activation</a></li>
<li class="chapter" data-level="2.2.2" data-path="the-artificial-neuron.html"><a href="the-artificial-neuron.html#implement-gradient-descent-for-this-linear-neuron-example"><i class="fa fa-check"></i><b>2.2.2</b> Implement gradient descent for this linear neuron example</a></li>
<li class="chapter" data-level="2.2.3" data-path="the-artificial-neuron.html"><a href="the-artificial-neuron.html#next-exercises-implement-classification-and-logistic-regression-when-time-permits"><i class="fa fa-check"></i><b>2.2.3</b> Next Exercises, Implement classification and logistic regression when time permits</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="learning-non-linear-functions.html"><a href="learning-non-linear-functions.html"><i class="fa fa-check"></i><b>3</b> Learning non-linear functions</a><ul>
<li class="chapter" data-level="3.1" data-path="learning-non-linear-functions.html"><a href="learning-non-linear-functions.html#learning-the-xor-function"><i class="fa fa-check"></i><b>3.1</b> Learning the XOR function</a></li>
<li class="chapter" data-level="3.2" data-path="learning-non-linear-functions.html"><a href="learning-non-linear-functions.html#the-universal-approximation-theorem"><i class="fa fa-check"></i><b>3.2</b> The Universal Approximation Theorem</a></li>
<li class="chapter" data-level="3.3" data-path="learning-non-linear-functions.html"><a href="learning-non-linear-functions.html#solving-xor-problem-a-neural-network-instead-of-a-single-neuron"><i class="fa fa-check"></i><b>3.3</b> Solving XOR problem a neural network instead of a single neuron</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="gradient-descent-algorithms.html"><a href="gradient-descent-algorithms.html"><i class="fa fa-check"></i><b>4</b> Gradient Descent Algorithms</a><ul>
<li class="chapter" data-level="4.1" data-path="gradient-descent-algorithms.html"><a href="gradient-descent-algorithms.html#improving-gradient-descent"><i class="fa fa-check"></i><b>4.1</b> Improving Gradient Descent</a><ul>
<li class="chapter" data-level="4.1.1" data-path="gradient-descent-algorithms.html"><a href="gradient-descent-algorithms.html#representative-sampling-for-stochastic-gradient-descent-sgd"><i class="fa fa-check"></i><b>4.1.1</b> Representative sampling for stochastic gradient descent (SGD)</a></li>
<li class="chapter" data-level="4.1.2" data-path="gradient-descent-algorithms.html"><a href="gradient-descent-algorithms.html#adaptive-batch-sizes"><i class="fa fa-check"></i><b>4.1.2</b> Adaptive batch sizes</a></li>
<li class="chapter" data-level="4.1.3" data-path="gradient-descent-algorithms.html"><a href="gradient-descent-algorithms.html#adaptive-learning-rates"><i class="fa fa-check"></i><b>4.1.3</b> Adaptive learning rates</a></li>
<li class="chapter" data-level="4.1.4" data-path="gradient-descent-algorithms.html"><a href="gradient-descent-algorithms.html#adaptive-loss-functions"><i class="fa fa-check"></i><b>4.1.4</b> Adaptive loss functions</a></li>
<li class="chapter" data-level="4.1.5" data-path="gradient-descent-algorithms.html"><a href="gradient-descent-algorithms.html#higher-order-derivatives"><i class="fa fa-check"></i><b>4.1.5</b> Higher order derivatives</a></li>
<li class="chapter" data-level="4.1.6" data-path="gradient-descent-algorithms.html"><a href="gradient-descent-algorithms.html#parallel-chains"><i class="fa fa-check"></i><b>4.1.6</b> Parallel chains</a></li>
<li class="chapter" data-level="4.1.7" data-path="gradient-descent-algorithms.html"><a href="gradient-descent-algorithms.html#branching-chains-at-each-step"><i class="fa fa-check"></i><b>4.1.7</b> Branching chains at each step</a></li>
<li class="chapter" data-level="4.1.8" data-path="gradient-descent-algorithms.html"><a href="gradient-descent-algorithms.html#identifying-pathological-topologies-to-stop-and-restart"><i class="fa fa-check"></i><b>4.1.8</b> Identifying pathological topologies to stop and restart</a></li>
<li class="chapter" data-level="4.1.9" data-path="gradient-descent-algorithms.html"><a href="gradient-descent-algorithms.html#hamiltonian-monte-carlo-based-method"><i class="fa fa-check"></i><b>4.1.9</b> Hamiltonian Monte Carlo based method</a></li>
<li class="chapter" data-level="4.1.10" data-path="gradient-descent-algorithms.html"><a href="gradient-descent-algorithms.html#section"><i class="fa fa-check"></i><b>4.1.10</b> …</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="automatic-differentiation.html"><a href="automatic-differentiation.html"><i class="fa fa-check"></i><b>5</b> Automatic Differentiation</a><ul>
<li class="chapter" data-level="5.1" data-path="automatic-differentiation.html"><a href="automatic-differentiation.html#performing-autodiff-in-julia"><i class="fa fa-check"></i><b>5.1</b> Performing Autodiff in Julia</a><ul>
<li class="chapter" data-level="5.1.1" data-path="automatic-differentiation.html"><a href="automatic-differentiation.html#using-flux"><i class="fa fa-check"></i><b>5.1.1</b> using Flux</a></li>
<li class="chapter" data-level="5.1.2" data-path="automatic-differentiation.html"><a href="automatic-differentiation.html#using-zygote"><i class="fa fa-check"></i><b>5.1.2</b> using Zygote</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li> &nbsp;&nbsp; &nbsp;&nbsp; &#169; Rajesh Talluri</li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Understanding Deep Learning</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="learning-non-linear-functions" class="section level1">
<h1><span class="header-section-number">Chapter 3</span> Learning non-linear functions</h1>
<p>We have seen from the previous chapter that a single neuron can be equipped to handle any generalized linear function by using the corresponding activation function. However, how about non linear functions? A standard non linear function that appears in many scenarios is the <strong>XOR</strong> or the <strong>exclusive or</strong> function.</p>
<div id="learning-the-xor-function" class="section level2">
<h2><span class="header-section-number">3.1</span> Learning the XOR function</h2>
<p>The XOR function looks like this</p>
<table class="table table-hover" style="width: auto !important; margin-left: auto; margin-right: auto;">
<thead>
<tr>
<th style="text-align:left;">
x1
</th>
<th style="text-align:left;">
x2
</th>
<th style="text-align:left;">
y
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
<span style="  font-style: italic;   color: white !important;border-radius: 4px; padding-right: 4px; padding-left: 4px; background-color: seagreen !important;">+1</span>
</td>
<td style="text-align:left;">
<span style="  font-style: italic;   color: white !important;border-radius: 4px; padding-right: 4px; padding-left: 4px; background-color: seagreen !important;">+1</span>
</td>
<td style="text-align:left;">
<span style=" font-weight: bold;    color: white !important;border-radius: 4px; padding-right: 4px; padding-left: 4px; background-color: tomato !important;">-1</span>
</td>
</tr>
<tr>
<td style="text-align:left;">
<span style="  font-style: italic;   color: white !important;border-radius: 4px; padding-right: 4px; padding-left: 4px; background-color: seagreen !important;">+1</span>
</td>
<td style="text-align:left;">
<span style=" font-weight: bold;    color: white !important;border-radius: 4px; padding-right: 4px; padding-left: 4px; background-color: tomato !important;">-1</span>
</td>
<td style="text-align:left;">
<span style="  font-style: italic;   color: white !important;border-radius: 4px; padding-right: 4px; padding-left: 4px; background-color: seagreen !important;">+1</span>
</td>
</tr>
<tr>
<td style="text-align:left;">
<span style=" font-weight: bold;    color: white !important;border-radius: 4px; padding-right: 4px; padding-left: 4px; background-color: tomato !important;">-1</span>
</td>
<td style="text-align:left;">
<span style="  font-style: italic;   color: white !important;border-radius: 4px; padding-right: 4px; padding-left: 4px; background-color: seagreen !important;">+1</span>
</td>
<td style="text-align:left;">
<span style="  font-style: italic;   color: white !important;border-radius: 4px; padding-right: 4px; padding-left: 4px; background-color: seagreen !important;">+1</span>
</td>
</tr>
<tr>
<td style="text-align:left;">
<span style=" font-weight: bold;    color: white !important;border-radius: 4px; padding-right: 4px; padding-left: 4px; background-color: tomato !important;">-1</span>
</td>
<td style="text-align:left;">
<span style=" font-weight: bold;    color: white !important;border-radius: 4px; padding-right: 4px; padding-left: 4px; background-color: tomato !important;">-1</span>
</td>
<td style="text-align:left;">
<span style=" font-weight: bold;    color: white !important;border-radius: 4px; padding-right: 4px; padding-left: 4px; background-color: tomato !important;">-1</span>
</td>
</tr>
</tbody>
</table>
<p>No linear classifier can fit this data</p>
<p><img src="Deeplearning_files/figure-html/unnamed-chunk-9-1.png" width="1152" /></p>
<blockquote>
<p>Can we use a single neuron to learn this data?</p>
</blockquote>
<p>Remember the general form of the model in a neuron is
<span class="math display">\[ y = \phi(w_0+w_1x_1+w_2x_2+\ldots+w_kx_k)
\]</span></p>
<p>so we could just use a non linear activation function <span class="math inline">\(\phi\)</span> to get the XOR function. There may be several activation functions that work. One example is</p>
<p><img src="Deeplearning_files/figure-html/unnamed-chunk-10-1.png" width="672" /></p>
<p>If we use the above function as the activation function for a single neuron, the classification boundaries would look as follows;</p>
<p><img src="Deeplearning_files/figure-html/unnamed-chunk-11-1.png" width="480" /></p>
<p>We were able to come up with this activation function because we exactly knew the output for every input. However, this will not be the case for other data sets. The thing we are trying to do is for the same neural network to do all possible tasks with the same activation function. We should be able to use more neurons with a universal activation function to classify this data.</p>
<p>The perfect(in what sense? common sense?) classification boundary for this XOR problem would be:</p>
<p><img src="Deeplearning_files/figure-html/unnamed-chunk-12-1.png" width="480" /></p>
<p>To achieve this classification boundary the activation function needs to be:</p>
<blockquote>
<p>… I cannot think of a way a single neuron can get this output using any activation function. The input to the activation function is a single value which is a linear combination of the inputs <span class="math inline">\(w_0+w_1x_1+w_2x_2\)</span>. However we transform this single value any mapping <span class="math inline">\(x\rightarrow \phi(x)\)</span> would not produce the classification boundaries above. This is because … any value of <span class="math inline">\(w_0+w_1x_1+w_2x_2\)</span> maps to two values depending on the values of <span class="math inline">\(x_1\)</span> and <span class="math inline">\(x_2\)</span>, which would not be a valid function,</p>
</blockquote>
<p>So…, why cant a neuron learn this function?
The issue is the linear combination of all the inputs in the neuron <span class="math inline">\(\sum_iw_ix_i\)</span>, which maps the inputs to a linear space where they cannot be separated.</p>
<p>Could a way to solve this problem with a single neuron, be to map to a linear combination of transformed inputs <span class="math inline">\(\sum_iw_if(x_i)\)</span>(No this will not work too be cause we need a multiplication/interaction term with <span class="math inline">\(x_1\)</span> and <span class="math inline">\(x_2\)</span> which is impossible to get with a linear combination).</p>
<blockquote>
<p>We do not want to use the multiplication operation in a neuron as there will be <span class="math inline">\(p!\)</span> ways to handle <span class="math inline">\(p\)</span> inputs, which will greatly complicate a neuron (we want to keep it as simple as possible and build using complex structures using simple structures.)</p>
</blockquote>
<blockquote>
<p>so the next question is… Which activation function can model any dataset. We have seen that linear activation function no matter what cannot create no linear classification boundaries so it is out of the question. It have to be something non linear.</p>
</blockquote>
</div>
<div id="the-universal-approximation-theorem" class="section level2">
<h2><span class="header-section-number">3.2</span> The Universal Approximation Theorem</h2>
<p>The universal approximation theorem states that, an artificial neural network with a single hidden layer can approximate any function <span class="math inline">\(f\)</span> in <span class="math inline">\(\mathbb{R}^n\)</span> to any desired degree of precision. A nice intuitive explanation, on how the universal approximation theorem works, is presented in <a href="http://neuralnetworksanddeeplearning.com/chap4.html">Michael Nielsen’s book</a>.</p>
<ol style="list-style-type: decimal">
<li>Not all activation functions may guarantee the approximation (e.g.linear activation functions cannot generate a nonlinear function), but most non-linear functions work.</li>
<li>The neural network can only approximate continuous functions.</li>
</ol>
<p>Mathematically any function can be approximated using:</p>
<p><span class="math display">\[\hat{f}(x)= \sum_{i = 1}^N \nu_i \phi(w_i^Tx)\]</span>
where, <span class="math inline">\(\nu_i\)</span> are the output weights and <span class="math inline">\(w_i\)</span> are the weights for each of the N neurons in the hidden layer.</p>
<p>and the universal approximation theorem guarantees that for any <span class="math inline">\(\epsilon\)</span> we can find a <span class="math inline">\(\hat{f}\)</span> depending on <span class="math inline">\(N\)</span></p>
<p><span class="math display">\[|f(x)-\hat{f}(x)|&lt;\epsilon\]</span></p>
<p>We will look at one simple example on how the universal approximation theorem works, by solving the XOR problem using a multiple neurons.</p>
</div>
<div id="solving-xor-problem-a-neural-network-instead-of-a-single-neuron" class="section level2">
<h2><span class="header-section-number">3.3</span> Solving XOR problem a neural network instead of a single neuron</h2>
<p>Some notation for representing neural networks.</p>
<p>The input layer does nothing it is just the inputs no neurons there.
Then the second layer is where the first layer of neurons gather all the inputs and transform them to another representation.
If there is only one neuron the output neuron is not needed. However if there are multiple neurons in the first layer an output neuron is needed to combine all the neuron outputs from the first layer, and produce the output in the required format based on the activation function.</p>
<p><img src="xorhidden.png"/></p>
<p>The above figure represents a neural network with 2 neuron in its hidden layer <span class="math inline">\(h_1\)</span> and <span class="math inline">\(h_2\)</span> and one output neuron <span class="math inline">\(y\)</span>.</p>
<button onclick="q1Function()">
Are you really thinking? Did you forget to ask questions?
</button>
<div id="q1" style="display:none;background-color:antiquewhite;">
<blockquote>
<p>If you havent asked the question on why we drew the neural network as above and not something like the ones below: then you are not reading this book correctly.</p>
</blockquote>
<p><img src="xor1.png"/>
<img src="xor2.png"/></p>
<p>These networks are also entirely possible and we do not know which one to use yet. We will get back to this after we learn how to train a neural network with multiple neurons.</p>
<script>
function q1Function() {
  var x = document.getElementById("q1");
  if (x.style.display === "none") {
    x.style.display = "block";
  } else {
    x.style.display = "none";
  }
}
</script>
</div>
<p>Let us derive the final functional form of the output.</p>
<p><span class="math display">\[ y = \phi_y(\nu_0 +\nu_1 h_1 +\nu_2 h_2)\]</span>
<span class="math display">\[ h_1 = \phi_{h_1}(w_{10}+w_{11}x_1+w_{12}x_2)\]</span>
<span class="math display">\[ h_2 = \phi_{h_2}(w_{20}+w_{21}x_1+w_{22}x_2)\]</span></p>
<p>So,
<span class="math display">\[ y = \phi_y(\nu_0 +\nu_1 \phi_{h_1}(w_{10}+w_{11}x_1+w_{12}x_2) +\nu_2 \phi_{h_2}(w_{20}+w_{21}x_1+w_{22}x_2))\]</span></p>
<p>Let us assume that all the activation functions are sigmoid functions.</p>
<p><span class="math display">\[ \phi = \frac{1}{1+e^{-x}} \]</span>
Then we can write the full functional form as:</p>
<p><span class="math display">\[ y = f(x_1,x_2) = \frac{1}{1+e^{
-(\nu_0 +\nu_1 \frac{1}{1+e^{-(w_{10}+w_{11}x_1+w_{12}x_2)}} +\nu_2 \frac{1}{1+e^{-(w_{20}+w_{21}x_1+w_{22}x_2)}}
)}}\]</span></p>
<p>Similar to when we trained the single neuron, the neuron has to learn more weights now <span class="math inline">\(\nu_{0,1,2},w_{10,11,12,20,21,22}\)</span>. We have to get the gradients with respect to the weights similar to what we did earlier.</p>
<blockquote>
<p>So….. what should we do</p>
</blockquote>
<ol style="list-style-type: decimal">
<li><p>Compute gradients by hand manually and code it. (Time consuming and impossibly complicated as our neural network increases in size)</p></li>
<li><p>There are algorithms to compute gradients numerically using automatic methods for any arbitrary function f. (Just needs computational power)</p></li>
</ol>
<blockquote>
<p>This is where we need some programming <span><img src="programming.png" height = "30"/></span> to calculate the gradients automatically. Take a detour to understand <a href="automatic-differentiation.html#automatic-differentiation">Automatic Differentiation</a> and come back here after understanding.</p>
</blockquote>
<p>Let us write the function to get the gradient of the above neural network function.</p>
<div class="sourceCode" id="cb3"><pre class="sourceCode julia"><code class="sourceCode julia"><a class="sourceLine" id="cb3-1" title="1">using Zygote</a>
<a class="sourceLine" id="cb3-2" title="2"><span class="co"># Define the function</span></a>
<a class="sourceLine" id="cb3-3" title="3">nnfun(θ, x) = <span class="fl">1</span>/(<span class="fl">1</span>+exp( -(θ.v[<span class="fl">1</span>] +θ.v[<span class="fl">2</span>]* <span class="fl">1</span>/(<span class="fl">1</span>+exp(-(θ.w[<span class="fl">1</span>,<span class="fl">1</span>] +θ.w[<span class="fl">1</span>,<span class="fl">2</span>]*x[<span class="fl">1</span>]+θ.w[<span class="fl">1</span>,<span class="fl">3</span>]*x[<span class="fl">2</span>])))+θ.v[<span class="fl">3</span>]*<span class="fl">1</span>/(<span class="fl">1</span>+exp(-(θ.w[<span class="fl">2</span>,<span class="fl">1</span>]+θ.w[<span class="fl">2</span>,<span class="fl">2</span>]*x[<span class="fl">1</span>]+θ.w[<span class="fl">2</span>,<span class="fl">3</span>]*x[<span class="fl">2</span>]))))))</a></code></pre></div>
<pre><code>## nnfun (generic function with 1 method)</code></pre>
<div class="sourceCode" id="cb5"><pre class="sourceCode julia"><code class="sourceCode julia"><a class="sourceLine" id="cb5-1" title="1"><span class="co"># Input x[1] and x[2]</span></a>
<a class="sourceLine" id="cb5-2" title="2">x = rand(<span class="fl">2</span>);</a>
<a class="sourceLine" id="cb5-3" title="3"><span class="co"># Initalize the weights θ[v,w]</span></a>
<a class="sourceLine" id="cb5-4" title="4">θ = (v = rand(<span class="fl">3</span>), w = rand(<span class="fl">2</span>,<span class="fl">3</span>))</a></code></pre></div>
<pre><code>## (v = [0.22158, 0.44004, 0.5872], w = [0.552103 0.974364 0.439959; 0.548627 0.550641 0.691859])</code></pre>
<div class="sourceCode" id="cb7"><pre class="sourceCode julia"><code class="sourceCode julia"><a class="sourceLine" id="cb7-1" title="1"><span class="co"># Take the gradient δθ for all parameters in θ</span></a>
<a class="sourceLine" id="cb7-2" title="2">δθ= gradient(θ -&gt; nnfun(θ,x), θ)[<span class="fl">1</span>]</a></code></pre></div>
<p>Now we can proceed to train this model using Gradient descent to find good weights for the neural network.</p>
<p>The gradient descent function is as follows</p>
<div class="sourceCode" id="cb8"><pre class="sourceCode julia"><code class="sourceCode julia"><a class="sourceLine" id="cb8-1" title="1">using Zygote</a>
<a class="sourceLine" id="cb8-2" title="2"><span class="co"># Generate data</span></a>
<a class="sourceLine" id="cb8-3" title="3"><span class="kw">function</span> dataxor(;n=<span class="fl">4</span>)</a>
<a class="sourceLine" id="cb8-4" title="4">    x = [-<span class="fl">1.0</span> -<span class="fl">1.0</span>; -<span class="fl">1.0</span> <span class="fl">1.0</span>; <span class="fl">1.0</span> -<span class="fl">1.0</span>;<span class="fl">1.0</span> <span class="fl">1.0</span>];</a>
<a class="sourceLine" id="cb8-5" title="5">    y = [<span class="fl">0.0</span>, <span class="fl">1.0</span>, <span class="fl">1.0</span>, <span class="fl">0.0</span>];</a>
<a class="sourceLine" id="cb8-6" title="6">    <span class="kw">return</span> (x,y)</a>
<a class="sourceLine" id="cb8-7" title="7"><span class="kw">end</span>;</a>
<a class="sourceLine" id="cb8-8" title="8"></a>
<a class="sourceLine" id="cb8-9" title="9">X,Y = dataxor();</a>
<a class="sourceLine" id="cb8-10" title="10"></a>
<a class="sourceLine" id="cb8-11" title="11"></a>
<a class="sourceLine" id="cb8-12" title="12"><span class="co"># neural network loss function</span></a>
<a class="sourceLine" id="cb8-13" title="13">nnfun(θ, x,y) = (y - <span class="fl">1</span>/(<span class="fl">1</span>+exp( -(θ[<span class="fl">1</span>][<span class="fl">1</span>] +θ[<span class="fl">1</span>][<span class="fl">2</span>]* <span class="fl">1</span>/(<span class="fl">1</span>+exp(-(θ[<span class="fl">2</span>][<span class="fl">1</span>,<span class="fl">1</span>] +θ[<span class="fl">2</span>][<span class="fl">1</span>,<span class="fl">2</span>]*x[<span class="fl">1</span>]+θ[<span class="fl">2</span>][<span class="fl">1</span>,<span class="fl">3</span>]*x[<span class="fl">2</span>])))+θ[<span class="fl">1</span>][<span class="fl">3</span>]*<span class="fl">1</span>/(<span class="fl">1</span>+exp(-(θ[<span class="fl">2</span>][<span class="fl">2</span>,<span class="fl">1</span>]+θ[<span class="fl">2</span>][<span class="fl">2</span>,<span class="fl">2</span>]*x[<span class="fl">1</span>]+θ[<span class="fl">2</span>][<span class="fl">2</span>,<span class="fl">3</span>]*x[<span class="fl">2</span>])))))))^<span class="fl">2</span>;</a>
<a class="sourceLine" id="cb8-14" title="14"></a>
<a class="sourceLine" id="cb8-15" title="15"><span class="co"># Perform gradient descent</span></a>
<a class="sourceLine" id="cb8-16" title="16"> <span class="kw">function</span> gradient_descent_nn(;Y::<span class="dt">Array</span>{<span class="dt">Float64</span>,<span class="fl">1</span>},X::<span class="dt">Array</span>{<span class="dt">Float64</span>,<span class="fl">2</span>}, α::<span class="dt">Float64</span> = <span class="fl">0.01</span>, epochs::<span class="dt">Int64</span> = <span class="fl">100</span>)</a>
<a class="sourceLine" id="cb8-17" title="17">     n,p = (size(X,<span class="fl">1</span>), size(X,<span class="fl">2</span>));    <span class="co"># get n and p from X</span></a>
<a class="sourceLine" id="cb8-18" title="18">     θ = (rand(p+<span class="fl">1</span>), rand(<span class="fl">2</span>,p+<span class="fl">1</span>));    <span class="co"># initialize θ: the weights vector</span></a>
<a class="sourceLine" id="cb8-19" title="19">     <span class="kw">for</span> i <span class="kw">in</span> <span class="fl">1</span>:epochs          <span class="co"># iterate set number of epochs for the weights to converge</span></a>
<a class="sourceLine" id="cb8-20" title="20">         δθ = (zeros(p+<span class="fl">1</span>), zeros(<span class="fl">2</span>,p+<span class="fl">1</span>));</a>
<a class="sourceLine" id="cb8-21" title="21">         <span class="kw">for</span> j <span class="kw">in</span> <span class="fl">1</span>:n</a>
<a class="sourceLine" id="cb8-22" title="22">             δθ = δθ .+ gradient(θ -&gt; nnfun(θ,X[j,:],Y[j]), θ)[<span class="fl">1</span>]; <span class="co"># Compute gradient for each input</span></a>
<a class="sourceLine" id="cb8-23" title="23">         <span class="kw">end</span></a>
<a class="sourceLine" id="cb8-24" title="24">         δθ = δθ./n;</a>
<a class="sourceLine" id="cb8-25" title="25">         θ = θ .- α.*δθ;              <span class="co"># update θ</span></a>
<a class="sourceLine" id="cb8-26" title="26"><span class="co">#         println(nnfun(θ,X[1,:],Y[1])+nnfun(θ,X[2,:],Y[2])+nnfun(θ,X[3,:],Y[3])+nnfun(θ,X[4,:],Y[4]))</span></a>
<a class="sourceLine" id="cb8-27" title="27">     <span class="kw">end</span></a>
<a class="sourceLine" id="cb8-28" title="28">     <span class="kw">return</span> θ;</a>
<a class="sourceLine" id="cb8-29" title="29"> <span class="kw">end</span>;</a>
<a class="sourceLine" id="cb8-30" title="30"></a>
<a class="sourceLine" id="cb8-31" title="31"></a>
<a class="sourceLine" id="cb8-32" title="32"><span class="co"># train the neural netowork</span></a>
<a class="sourceLine" id="cb8-33" title="33">nepochs = <span class="fl">1000</span>;</a>
<a class="sourceLine" id="cb8-34" title="34"><span class="co">#θ = gradient_descent_nn(Y = Y, X = X, α = 0.35, epochs = nepochs);</span></a></code></pre></div>
<p>This is the final output function the neural network has trained to output based on the input data.
<img src="xorbound.png"/></p>
<button onclick="q2Function()">
Click to look at the how the neural network trains with epochs
</button>
<div id="q2" style="display:none;">
<p><img src="xor1.gif"/></p>
</div>
<script>
function q2Function() {
  var x = document.getElementById("q2");
  if (x.style.display === "none") {
    x.style.display = "block";
  } else {
    x.style.display = "none";
  }
}
</script>
</div>
<blockquote>
<p>So, why is it that the neural network trained to get this function. Why did it not fit a more complex surface.</p>
</blockquote>
<blockquote>
<p>Are there any limitations to complexity?</p>
</blockquote>
<p>Yes, the neural network can only generate a surface of this complexity based on the network architecture with 2 hidden neurons. As we learned in the universal approximation thoerem, Any function can be approximated if we use more neurons. So let us use more neurons and see how the surface complexity changes.</p>
<p>This is important because this is how you would determine the number of neurons in your neural network.</p>
<p>Trying out 3,4,5 hidden neurons.</p>
<p>The results seem similar and getting the same solutions even for higher number of neurons. This may be because of the limited data with only 4 data points.</p>
<blockquote>
<p>Ok, So what do I want to understand /do next?</p>
</blockquote>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="the-artificial-neuron.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="gradient-descent-algorithms.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": true,
"facebook": false,
"twitter": true,
"google": false,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"download": null,
"toc": {
"collapse": "none"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:" && /^https?:/.test(src))
      src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
