[
["automatic-differentiation.html", "5 Automatic Differentiation 5.1 Performing Autodiff in Julia", " 5 Automatic Differentiation Automatic differentiation is a set of techniques to numerically evaluate the derivative of a function specified by a computer program . 5.1 Performing Autodiff in Julia 5.1.1 using Flux using Flux; f(x) = 3x^2 + 2x + 1; df(x) = Tracker.gradient(f, x; nest = true)[1]; # df/dx = 6x + 2 ## df (generic function with 1 method) df(2) d2f(x) = Tracker.gradient(df, x; nest = true)[1]; # d²f/dx² = 6 ## d2f (generic function with 1 method) d2f(2) Using a more complex example fo a neural network with 2 neurons f(x_1,x_2) = 1/(1+exp( -(v_0 +v_1* 1/(1+exp(-(w_10 +w_11*x_1+w_12*x_2)))+v_2*1/(1+exp(-(w_20+w_21*x_1+w_22*x_2)))))); v_0,v_1,v_2,w_10,w_11,w_12,w_20,w_21,w_22 = [param(1.0),param(1.0),param(1.0),param(1.0),param(1.0),param(1.0),param(1.0),param(1.0),param(1.0)]; grads = Tracker.gradient(() -&gt; f(4,6), params(v_0,v_1,v_2,w_10,w_11,w_12,w_20,w_21,w_22)); grads.grads 5.1.2 using Zygote using Zygote linear(θ, x) = θ[:W] * x .+ θ[:b] ## linear (generic function with 1 method) x = rand(5); θ = (W = rand(2, 5), b = rand(2)) ## (W = [0.4706768045985694 0.047064110128199665 … 0.5715106861239425 0.7893798898928712; 0.9515224075955029 0.2654790301164831 … 0.895009608566655 0.5565304822918382], b = [0.8191635228551253, 0.8947091657517481]) δθ= gradient(θ -&gt; sum(linear(θ, x)), θ)[1] ## (W = [0.3267766266200214 0.8608717344908234 … 0.48620163767511637 0.7732045829451226; 0.3267766266200214 0.8608717344908234 … 0.48620163767511637 0.7732045829451226], b = [1.0, 1.0]) "]
]
