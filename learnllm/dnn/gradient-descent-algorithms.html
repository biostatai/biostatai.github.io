<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 4 Gradient Descent Algorithms | Understanding Deep Learning</title>
  <meta name="description" content="Chapter 4 Gradient Descent Algorithms | Understanding Deep Learning" />
  <meta name="generator" content="bookdown 0.12 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 4 Gradient Descent Algorithms | Understanding Deep Learning" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="Chapter 4 Gradient Descent Algorithms | Understanding Deep Learning" />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 4 Gradient Descent Algorithms | Understanding Deep Learning" />
  
  <meta name="twitter:description" content="Chapter 4 Gradient Descent Algorithms | Understanding Deep Learning" />
  

<meta name="author" content="Rajesh Talluri, PhD" />


<meta name="date" content="2019-09-12" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="learning-non-linear-functions.html">
<link rel="next" href="automatic-differentiation.html">
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />







<script src="libs/kePrint-0.0.1/kePrint.js"></script>


<style type="text/css">
a.sourceLine { display: inline-block; line-height: 1.25; }
a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }
a.sourceLine:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
a.sourceLine { text-indent: -1em; padding-left: 1em; }
}
pre.numberSource a.sourceLine
  { position: relative; left: -4em; }
pre.numberSource a.sourceLine::before
  { content: attr(title);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; pointer-events: all; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  }
@media screen {
a.sourceLine::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Understanding Deep Learning</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a></li>
<li class="chapter" data-level="1" data-path="start-asking-questions.html"><a href="start-asking-questions.html"><i class="fa fa-check"></i><b>1</b> Start asking questions</a><ul>
<li class="chapter" data-level="1.1" data-path="start-asking-questions.html"><a href="start-asking-questions.html#what-is-deep-learning"><i class="fa fa-check"></i><b>1.1</b> What is deep learning?</a></li>
<li class="chapter" data-level="1.2" data-path="start-asking-questions.html"><a href="start-asking-questions.html#what-are-artificial-neural-networks"><i class="fa fa-check"></i><b>1.2</b> What are artificial neural networks?</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="the-artificial-neuron.html"><a href="the-artificial-neuron.html"><i class="fa fa-check"></i><b>2</b> The artificial neuron</a><ul>
<li class="chapter" data-level="2.1" data-path="the-artificial-neuron.html"><a href="the-artificial-neuron.html#what-can-a-single-neuron-accomplish"><i class="fa fa-check"></i><b>2.1</b> What can a single neuron accomplish?</a><ul>
<li class="chapter" data-level="2.1.1" data-path="the-artificial-neuron.html"><a href="the-artificial-neuron.html#linear-regression"><i class="fa fa-check"></i><b>2.1.1</b> Linear Regression</a></li>
<li class="chapter" data-level="2.1.2" data-path="the-artificial-neuron.html"><a href="the-artificial-neuron.html#logistic-regression"><i class="fa fa-check"></i><b>2.1.2</b> Logistic regression</a></li>
<li class="chapter" data-level="2.1.3" data-path="the-artificial-neuron.html"><a href="the-artificial-neuron.html#linear-classification"><i class="fa fa-check"></i><b>2.1.3</b> Linear Classification</a></li>
<li class="chapter" data-level="2.1.4" data-path="the-artificial-neuron.html"><a href="the-artificial-neuron.html#other-activation-functions"><i class="fa fa-check"></i><b>2.1.4</b> Other activation functions</a></li>
<li class="chapter" data-level="2.1.5" data-path="the-artificial-neuron.html"><a href="the-artificial-neuron.html#biological-activation-functions"><i class="fa fa-check"></i><b>2.1.5</b> Biological Activation functions</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="the-artificial-neuron.html"><a href="the-artificial-neuron.html#how-does-a-neuron-learn"><i class="fa fa-check"></i><b>2.2</b> How does a neuron learn?</a><ul>
<li class="chapter" data-level="2.2.1" data-path="the-artificial-neuron.html"><a href="the-artificial-neuron.html#estimation-of-weights-for-a-single-neuron-with-identity-activation"><i class="fa fa-check"></i><b>2.2.1</b> Estimation of weights for a single neuron with identity activation</a></li>
<li class="chapter" data-level="2.2.2" data-path="the-artificial-neuron.html"><a href="the-artificial-neuron.html#implement-gradient-descent-for-this-linear-neuron-example"><i class="fa fa-check"></i><b>2.2.2</b> Implement gradient descent for this linear neuron example</a></li>
<li class="chapter" data-level="2.2.3" data-path="the-artificial-neuron.html"><a href="the-artificial-neuron.html#next-exercises-implement-classification-and-logistic-regression-when-time-permits"><i class="fa fa-check"></i><b>2.2.3</b> Next Exercises, Implement classification and logistic regression when time permits</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="learning-non-linear-functions.html"><a href="learning-non-linear-functions.html"><i class="fa fa-check"></i><b>3</b> Learning non-linear functions</a><ul>
<li class="chapter" data-level="3.1" data-path="learning-non-linear-functions.html"><a href="learning-non-linear-functions.html#learning-the-xor-function"><i class="fa fa-check"></i><b>3.1</b> Learning the XOR function</a></li>
<li class="chapter" data-level="3.2" data-path="learning-non-linear-functions.html"><a href="learning-non-linear-functions.html#the-universal-approximation-theorem"><i class="fa fa-check"></i><b>3.2</b> The Universal Approximation Theorem</a></li>
<li class="chapter" data-level="3.3" data-path="learning-non-linear-functions.html"><a href="learning-non-linear-functions.html#solving-xor-problem-a-neural-network-instead-of-a-single-neuron"><i class="fa fa-check"></i><b>3.3</b> Solving XOR problem a neural network instead of a single neuron</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="gradient-descent-algorithms.html"><a href="gradient-descent-algorithms.html"><i class="fa fa-check"></i><b>4</b> Gradient Descent Algorithms</a><ul>
<li class="chapter" data-level="4.1" data-path="gradient-descent-algorithms.html"><a href="gradient-descent-algorithms.html#improving-gradient-descent"><i class="fa fa-check"></i><b>4.1</b> Improving Gradient Descent</a><ul>
<li class="chapter" data-level="4.1.1" data-path="gradient-descent-algorithms.html"><a href="gradient-descent-algorithms.html#representative-sampling-for-stochastic-gradient-descent-sgd"><i class="fa fa-check"></i><b>4.1.1</b> Representative sampling for stochastic gradient descent (SGD)</a></li>
<li class="chapter" data-level="4.1.2" data-path="gradient-descent-algorithms.html"><a href="gradient-descent-algorithms.html#adaptive-batch-sizes"><i class="fa fa-check"></i><b>4.1.2</b> Adaptive batch sizes</a></li>
<li class="chapter" data-level="4.1.3" data-path="gradient-descent-algorithms.html"><a href="gradient-descent-algorithms.html#adaptive-learning-rates"><i class="fa fa-check"></i><b>4.1.3</b> Adaptive learning rates</a></li>
<li class="chapter" data-level="4.1.4" data-path="gradient-descent-algorithms.html"><a href="gradient-descent-algorithms.html#adaptive-loss-functions"><i class="fa fa-check"></i><b>4.1.4</b> Adaptive loss functions</a></li>
<li class="chapter" data-level="4.1.5" data-path="gradient-descent-algorithms.html"><a href="gradient-descent-algorithms.html#higher-order-derivatives"><i class="fa fa-check"></i><b>4.1.5</b> Higher order derivatives</a></li>
<li class="chapter" data-level="4.1.6" data-path="gradient-descent-algorithms.html"><a href="gradient-descent-algorithms.html#parallel-chains"><i class="fa fa-check"></i><b>4.1.6</b> Parallel chains</a></li>
<li class="chapter" data-level="4.1.7" data-path="gradient-descent-algorithms.html"><a href="gradient-descent-algorithms.html#branching-chains-at-each-step"><i class="fa fa-check"></i><b>4.1.7</b> Branching chains at each step</a></li>
<li class="chapter" data-level="4.1.8" data-path="gradient-descent-algorithms.html"><a href="gradient-descent-algorithms.html#identifying-pathological-topologies-to-stop-and-restart"><i class="fa fa-check"></i><b>4.1.8</b> Identifying pathological topologies to stop and restart</a></li>
<li class="chapter" data-level="4.1.9" data-path="gradient-descent-algorithms.html"><a href="gradient-descent-algorithms.html#hamiltonian-monte-carlo-based-method"><i class="fa fa-check"></i><b>4.1.9</b> Hamiltonian Monte Carlo based method</a></li>
<li class="chapter" data-level="4.1.10" data-path="gradient-descent-algorithms.html"><a href="gradient-descent-algorithms.html#section"><i class="fa fa-check"></i><b>4.1.10</b> …</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="automatic-differentiation.html"><a href="automatic-differentiation.html"><i class="fa fa-check"></i><b>5</b> Automatic Differentiation</a><ul>
<li class="chapter" data-level="5.1" data-path="automatic-differentiation.html"><a href="automatic-differentiation.html#performing-autodiff-in-julia"><i class="fa fa-check"></i><b>5.1</b> Performing Autodiff in Julia</a><ul>
<li class="chapter" data-level="5.1.1" data-path="automatic-differentiation.html"><a href="automatic-differentiation.html#using-flux"><i class="fa fa-check"></i><b>5.1.1</b> using Flux</a></li>
<li class="chapter" data-level="5.1.2" data-path="automatic-differentiation.html"><a href="automatic-differentiation.html#using-zygote"><i class="fa fa-check"></i><b>5.1.2</b> using Zygote</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li> &nbsp;&nbsp; &nbsp;&nbsp; &#169; Rajesh Talluri</li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Understanding Deep Learning</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="gradient-descent-algorithms" class="section level1">
<h1><span class="header-section-number">Chapter 4</span> Gradient Descent Algorithms</h1>
<div>
<img src="c3.gif" />
<center>
<div>
Optimal learning rate
</div>
</center>
</div>
<div>
<img src="c2.gif"/>
<center>
<div>
Higher learning rate
</div>
</center>
</div>
<div>
<img src="c1.gif" />
<center>
<div>
Unstable learning rate
</div>
</center>
</div>
<div>
<img src="c4.gif" />
<center>
<div>
Slow learning rate
</div>
</center>
</div>
<div id="improving-gradient-descent" class="section level2">
<h2><span class="header-section-number">4.1</span> Improving Gradient Descent</h2>
<p>I am putting together a list of stuff in my head right now, I will write down more detailed ideas as we go further.</p>
<p>Things you could change to improve gradient descent</p>
<div id="representative-sampling-for-stochastic-gradient-descent-sgd" class="section level3">
<h3><span class="header-section-number">4.1.1</span> Representative sampling for stochastic gradient descent (SGD)</h3>
<p>This aspect covers which samples are used to estimate the gradient in SGD</p>
<blockquote>
<p>What is the best subset to sample for gradient descent?</p>
</blockquote>
<p>To answer this question we could use concepts like importance sampling etc, to get the best subset for computing gradient in SGD, instead of sampling randomly. We could derive the estimated uncertainty in the estimated gradient (because of sampling 10% of the data the variance will be higher), and use that information in SGD to improve its performance. Some bootstrap theory to reduce bias in sampled gradient would help.</p>
</div>
<div id="adaptive-batch-sizes" class="section level3">
<h3><span class="header-section-number">4.1.2</span> Adaptive batch sizes</h3>
<blockquote>
<p>What is the best subset size to sample for gradient descent?</p>
</blockquote>
<p>This question is very related to the above question of best subset. So, depending on the sampling strategy our best batch size would vary. And this batch size could be adjusted as we dive deeper into the loss function.</p>
<p>One basic intuitive strategy is to use lower batch sizes at the start when the loss is higher,</p>
<ol style="list-style-type: decimal">
<li>This is to have more randomness/variability to escape shallow local minima at higher losses.</li>
<li>You don’t need much information as the loss will be higher even with very low number of samples, which makes for efficient computation.</li>
<li>We need more information (larger batch size) about the gradient when we are close to the minima, to not waste time randomly going about because we picked a smaller batch size.</li>
</ol>
</div>
<div id="adaptive-learning-rates" class="section level3">
<h3><span class="header-section-number">4.1.3</span> Adaptive learning rates</h3>
<p>As it is the most used technique to adaptively learn the gradient, most of the things discussed here can be incorporated into learning rates.</p>
<p>Some concepts for adapting learning rates:</p>
<ol style="list-style-type: decimal">
<li>Momentum- widely explored but maybe some more intuition can be used to improve.</li>
<li><p>Adaptive momentum- some ideas to implement adaptive momentum based on topology learned from descent history.</p></li>
<li><p>Why not look at where you went, and take a step back when the gradient step was not efficient compared to the history/what you expected. Kind of like acception/rejection of gradient steps and then propose a new learning rate when things are inefficient.</p></li>
</ol>
<p>… some more vague ideas</p>
</div>
<div id="adaptive-loss-functions" class="section level3">
<h3><span class="header-section-number">4.1.4</span> Adaptive loss functions</h3>
<p>This is based on the idea that changing the loss function would affect the gradient descent.</p>
<blockquote>
<p>Why would we want to do that.</p>
</blockquote>
<ol style="list-style-type: decimal">
<li><p>At higher losses we only need an approximate smoother version of the loss function, because we risk settling into a very local shallow minima at higher losses because the loss function is very wavy at higher dimensions. If there is a way to make the shallow minima even shallow at higher losses, this would immensely help with avoiding those local minima which are not good solutions. A log like transformation of the loss function at higher losses which squashes the values together would be apt for this kind of problem.</p></li>
<li><p>At lower losses, we have already settled into one of the possible local minima solutions. Here we could scale the gradient up/leave it alone depending on the topology using another transformation.</p></li>
</ol>
<p>In essence we could get better, faster and stable learning algorithms if we could adapt our loss functions which calculate the gradients depending on the loss.</p>
</div>
<div id="higher-order-derivatives" class="section level3">
<h3><span class="header-section-number">4.1.5</span> Higher order derivatives</h3>
<p>Higher order derivatives are harder to compute. They also have issues like converging to saddle points etc. However, it should be Ok to adaptively compute higher order derivatives and use that info when we algorithmically estimate that more information would help at this particular gradient step. This would be likely to avoid pathological topologies from disorienting the gradient.</p>
</div>
<div id="parallel-chains" class="section level3">
<h3><span class="header-section-number">4.1.6</span> Parallel chains</h3>
<p>I did not see much parallel chains in SGD so I was wondering about mixing and tempering chains like in MCMC.</p>
</div>
<div id="branching-chains-at-each-step" class="section level3">
<h3><span class="header-section-number">4.1.7</span> Branching chains at each step</h3>
<p>This idea is to randomly sample a number of points in a fixed/random distance from the current point. This would help use determine the best direction for the gradient to move. The best direction may not the be the steepest descent which sometimes leads to oscillations. This method can be used independently of the gradient or in addition to the gradient to adjust the direction of the gradient to a more optimal direction based on additional information.</p>
</div>
<div id="identifying-pathological-topologies-to-stop-and-restart" class="section level3">
<h3><span class="header-section-number">4.1.8</span> Identifying pathological topologies to stop and restart</h3>
<p>Some algorithmic way to identify pathological topologies. Like things being done in stan. It identifies pathological topologies using some diagnostic algorithms. This would help to provide gradient descent additional ways to escape or traverse these topologies when encountered.</p>
</div>
<div id="hamiltonian-monte-carlo-based-method" class="section level3">
<h3><span class="header-section-number">4.1.9</span> Hamiltonian Monte Carlo based method</h3>
<p>One idea is to interleave a HMC move after every descent/ at least interleave some HMC moves in gradient descent.</p>
<p>This may not really improve anything as we may move away from a better minima sometimes.</p>
<p>We move one step at the same energy randomly to explore from a different point similar to the one it is now.</p>
<p>However there is one other thing that is important with HMC.</p>
<p>The thing we need to understand is that the final trained model weights are not a minima generally. This is because local minima would over fit the model. Instead we are going to stop at a particular number of epochs when the test error stops decreasing. This would not likely correspond to any minima.</p>
<p>At this step we start HMC and find out solutions with similar loss. This move could identify some of multiple solutions to the nonidentifiable neural network which have similar accuracies/loss. We could then evaluate these solutions by model averaging or weighted ensemble of the solutions we get from HMC moves from the solution point. However each of these solutions may work better for only some subsets of data. By ensembling them we could possibly create a better model with improved accuracy and no overfitting.</p>
</div>
<div id="section" class="section level3">
<h3><span class="header-section-number">4.1.10</span> …</h3>

</div>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="learning-non-linear-functions.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="automatic-differentiation.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": true,
"facebook": false,
"twitter": true,
"google": false,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"download": null,
"toc": {
"collapse": "none"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:" && /^https?:/.test(src))
      src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
